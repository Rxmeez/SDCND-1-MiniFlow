# MiniFlow

Applying the concepts of regression, classification and neural network, from high level concepts to the low level details by building my own neural network libary.

This reason for doing this is to demystify two concepts at the heart of neural networks - backpropagation and differentiable graphs.

---

Backpropagation is the process by which neural networks update the weights of the network over time.

Differentiable graphs are graphs where the nodes are differentiable functions. They are also useful as visual aids for understanding and calculating complicated derivatives. This is the fundamental abstraction of TensorFlow - It's a framework for creating differentiable graphs.

With graphs and backpropagation, you will be able to create my own nodes and properly compute the derivatives. Even more importantly, I will be able to think and reason in terms of these graphs.
